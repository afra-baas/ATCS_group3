# from transformers import LlamaForCausalLM, LlamaTokenizer

# tokenizer = LlamaTokenizer.from_pretrained("/output/path")
# model = LlamaForCausalLM.from_pretrained("/output/path")


# from transformers import LlamaModel, LlamaConfig

# # Initializing a LLaMA llama-7b style configuration
# configuration = LlamaConfig()

# # Initializing a model from the llama-7b style configuration
# model = LlamaModel(configuration)

# # Accessing the model configuration
# configuration = model.config
