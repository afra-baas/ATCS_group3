----------- llama SA --------------
Loading model huggyllama/llama-7b
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
Model huggyllama/llama-7b loaded
Moving model to cuda
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 31.37it/s]
MARC dataset loaded
len dataset  4
Batch: 0 , batch size: 2, sample_size: 4
inputs  {'input_ids': tensor([[32000, 32000, 32000, 32000,     1,  1334,   674,  2367,   366,   263,
           731,   310, 11994,   385,  1881, 10541,   322,   263, 22320,   719,
         29889,   887,   881,  1234,   278, 22320,   719,  2729,   373,   278,
          1881, 10541, 15017,   304,   278, 11994,  4944, 29889, 29871,    13,
          2611,   582,  1953, 29901, 24948,   592,   565,   278, 19688,   310,
           445,  9076,   338,  6374, 29871,    13,  2080, 10541, 29901, 18064,
          7575,  1919,  1457,  9514,  2107,   869, 29956,  8417,  1319, 29871,
            13,  7808,   719, 29901,  4874,   470,   694, 29973, 29871,    13,
         12011, 29901, 29871],
        [    1,  1334,   674,  2367,   366,   263,   731,   310, 11994,   385,
          1881, 10541,   322,   263, 22320,   719, 29889,   887,   881,  1234,
           278, 22320,   719,  2729,   373,   278,  1881, 10541, 15017,   304,
           278, 11994,  4944, 29889, 29871,    13,  2611,   582,  1953, 29901,
         24948,   592,   565,   278, 19688,   310,   445,  9076,   338,  6374,
         29871,    13,  2080, 10541, 29901,  7027, 11029, 29889,   450,   503,
         29875,  2496,   338, 10597,   322,   380,   332,  4518, 29889, 29871,
            13,  7808,   719, 29901,  4874,   470,   694, 29973, 29871,    13,
         12011, 29901, 29871]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
Traceback (most recent call last):
  File "/home/lcur1101/ATCS_group3/src/main.py", line 85, in <module>
    pipeline(args)
  File "/home/lcur1101/ATCS_group3/src/main.py", line 43, in pipeline
    answers_probs_batch, pred_answer_batch = LM(
  File "/home/lcur1101/ATCS_group3/src/models/model.py", line 57, in __call__
    outputs = self.model(
  File "/home/lcur1101/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lcur1101/.conda/envs/dl2022/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 687, in forward
    outputs = self.model(
  File "/home/lcur1101/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lcur1101/.conda/envs/dl2022/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 530, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/home/lcur1101/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lcur1101/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/lcur1101/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
srun: error: r33n6: task 0: Exited with exit code 1
