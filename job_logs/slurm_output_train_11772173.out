Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
----------- bloom SA --------------
Loading model bigscience/bloom-560m
Model bigscience/bloom-560m loaded
Moving model to cuda
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 30.02it/s]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
MARC dataset loaded
len dataset  4
Batch: 0 , batch size: 2, sample_size: 4
inputs  {'input_ids': tensor([[  7994,   2152,  10665,   1152,    267,   1907,    461,  45018,    660,
           9437,  42932,    530,    267,    344,  16796,     17,   5568,   3403,
          12300,    368,    344,  16796,  11173,    664,    368,   9437,  42932,
          10813,    427,    368,  45018,  15984,  10209,  26541,  78348,     29,
          95292,   1074,   1320,    368,  35591,    461,   1119,  17953,    632,
          18121,   5306,   8684,  42932,     29,    473,  19134,  20084,  13596,
           2048,  26942,    530,   1542,   3784,   3936,   4054,    613,   7112,
           8621,     17,   7702,   1119,   6036,   4689,   1130,   2909,    613,
           1074,     17,    473,  57247,  12490,    427,  36789,     17,    473,
           1542,   5322,    461, 222599,   1776,  45858,    361,   3331,  24010,
             15,   3331,  45712,     78,   1336,    530,   3331, 192574,  19028,
             17,   7702,   3262,    473,   5718,   1119,    664,    718,   1620,
          85646,      4,  30497,     15,   6582,   4731,    267,  12442,  17405,
            461,  10276,    718,  22849,  41097,   5276,    664,  19427,      4,
            473,   1542,  11705,   3866,    861,  15591,   1002,   2914,    461,
           3808,  26942,   6880,  10209,    306,  16796,     29,  34474,    791,
            654,  56543,  68365,     29,    210],
        [     3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,      3,      3,      3,      3,      3,      3,      3,      3,
              3,   7994,   2152,  10665,   1152,    267,   1907,    461,  45018,
            660,   9437,  42932,    530,    267,    344,  16796,     17,   5568,
           3403,  12300,    368,    344,  16796,  11173,    664,    368,   9437,
          42932,  10813,    427,    368,  45018,  15984,  10209,  26541,  78348,
             29,  95292,   1074,   1320,    368,  35591,    461,   1119,  17953,
            632,  18121,   5306,   8684,  42932,     29,  58133,     80,   3226,
         139641,  86918,   3968,    415,  17405,    461, 100391,    718,    530,
         109676,  26489,  27056,   5306,    306,  16796,     29,  34474,    791,
            654,  56543,  68365,     29,    210]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1]])}
{'input_ids': [18260], 'attention_mask': [1]}
{'input_ids': [1936], 'attention_mask': [1]}
pred_answer ['yes', 'yes']
pred_answer ['yes', 'yes'] , label: ('no', 'no')
Batch: 1 , batch size: 2, sample_size: 4
inputs  {'input_ids': tensor([[ 7994,  2152, 10665,  1152,   267,  1907,   461, 45018,   660,  9437,
         42932,   530,   267,   344, 16796,    17,  5568,  3403, 12300,   368,
           344, 16796, 11173,   664,   368,  9437, 42932, 10813,   427,   368,
         45018, 15984, 10209, 26541, 78348,    29, 95292,  1074,  1320,   368,
         35591,   461,  1119, 17953,   632, 18121,  5306,  8684, 42932,    29,
         36534, 23898,    17,  1387, 60991,  1297,   632, 50825,   530,   960,
           323, 18909, 10209,   306, 16796,    29, 34474,   791,   654, 56543,
         68365,    29,   210],
        [    3,     3,  7994,  2152, 10665,  1152,   267,  1907,   461, 45018,
           660,  9437, 42932,   530,   267,   344, 16796,    17,  5568,  3403,
         12300,   368,   344, 16796, 11173,   664,   368,  9437, 42932, 10813,
           427,   368, 45018, 15984, 10209, 26541, 78348,    29, 95292,  1074,
          1320,   368, 35591,   461,  1119, 17953,   632, 18121,  5306,  8684,
         42932,    29, 93269, 35378,   630,  8602, 73832, 10087,   503,    58,
         24127,  5086,  5306,   306, 16796,    29, 34474,   791,   654, 56543,
         68365,    29,   210]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1],
        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1]])}
{'input_ids': [18260], 'attention_mask': [1]}
{'input_ids': [1936], 'attention_mask': [1]}
pred_answer ['yes', 'yes']
pred_answer ['yes', 'yes'] , label: ('yes', 'yes')
acc:  0.5
