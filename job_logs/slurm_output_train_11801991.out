Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
****Start Time: 2023-05-22_01-01-47
Loading model bigscience/bloom-560m
Model bigscience/bloom-560m loaded
Available device is cuda
Model device: cuda:0
----------- 42 en bigscience/bloom-560m SA active 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  3.57it/s]100%|██████████| 3/3 [00:00<00:00,  9.27it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 0 and batchsize 16: 0:03:12.892051
path ['42', 'en', 'bloom', 'SA', 'active', 'prompt_id_0']
----------- 42 en bigscience/bloom-560m SA active 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 274.85it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 1 and batchsize 16: 0:03:42.117532
path ['42', 'en', 'bloom', 'SA', 'active', 'prompt_id_1']
----------- 42 en bigscience/bloom-560m SA active 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 340.22it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 2 and batchsize 16: 0:03:09.875261
path ['42', 'en', 'bloom', 'SA', 'active', 'prompt_id_2']
----------- 42 en bigscience/bloom-560m SA active 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 206.93it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 3 and batchsize 16: 0:03:37.543215
path ['42', 'en', 'bloom', 'SA', 'active', 'prompt_id_3']
----------- 42 en bigscience/bloom-560m SA active 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 353.95it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 4 and batchsize 16: 0:03:38.527715
path ['42', 'en', 'bloom', 'SA', 'active', 'prompt_id_4']
----------- 42 en bigscience/bloom-560m SA active 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 193.85it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 5 and batchsize 16: 0:03:34.002684
path ['42', 'en', 'bloom', 'SA', 'active', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloom-560m SA passive 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 339.56it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 0 and batchsize 16: 0:03:35.086480
path ['42', 'en', 'bloom', 'SA', 'passive', 'prompt_id_0']
----------- 42 en bigscience/bloom-560m SA passive 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 281.70it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 1 and batchsize 16: 0:03:34.992705
path ['42', 'en', 'bloom', 'SA', 'passive', 'prompt_id_1']
----------- 42 en bigscience/bloom-560m SA passive 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 351.70it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 2 and batchsize 16: 0:03:04.229620
path ['42', 'en', 'bloom', 'SA', 'passive', 'prompt_id_2']
----------- 42 en bigscience/bloom-560m SA passive 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 338.01it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 3 and batchsize 16: 0:03:04.343612
path ['42', 'en', 'bloom', 'SA', 'passive', 'prompt_id_3']
----------- 42 en bigscience/bloom-560m SA passive 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 345.46it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 4 and batchsize 16: 0:03:06.183681
path ['42', 'en', 'bloom', 'SA', 'passive', 'prompt_id_4']
----------- 42 en bigscience/bloom-560m SA passive 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 189.13it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 5 and batchsize 16: 0:03:32.783499
path ['42', 'en', 'bloom', 'SA', 'passive', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloom-560m SA auxiliary 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 305.14it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 0 and batchsize 16: 0:03:34.073487
path ['42', 'en', 'bloom', 'SA', 'auxiliary', 'prompt_id_0']
----------- 42 en bigscience/bloom-560m SA auxiliary 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 371.56it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 1 and batchsize 16: 0:03:35.648641
path ['42', 'en', 'bloom', 'SA', 'auxiliary', 'prompt_id_1']
----------- 42 en bigscience/bloom-560m SA auxiliary 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 332.93it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 2 and batchsize 16: 0:03:34.418335
path ['42', 'en', 'bloom', 'SA', 'auxiliary', 'prompt_id_2']
----------- 42 en bigscience/bloom-560m SA auxiliary 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 360.21it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 3 and batchsize 16: 0:03:34.489760
path ['42', 'en', 'bloom', 'SA', 'auxiliary', 'prompt_id_3']
----------- 42 en bigscience/bloom-560m SA auxiliary 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 339.69it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 4 and batchsize 16: 0:03:35.639800
path ['42', 'en', 'bloom', 'SA', 'auxiliary', 'prompt_id_4']
----------- 42 en bigscience/bloom-560m SA auxiliary 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 350.15it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 5 and batchsize 16: 0:03:23.547421
path ['42', 'en', 'bloom', 'SA', 'auxiliary', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloom-560m SA modal 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 343.01it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 0 and batchsize 16: 0:03:33.374690
path ['42', 'en', 'bloom', 'SA', 'modal', 'prompt_id_0']
----------- 42 en bigscience/bloom-560m SA modal 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 336.08it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 1 and batchsize 16: 0:03:34.026944
path ['42', 'en', 'bloom', 'SA', 'modal', 'prompt_id_1']
----------- 42 en bigscience/bloom-560m SA modal 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 383.02it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 2 and batchsize 16: 0:03:33.290220
path ['42', 'en', 'bloom', 'SA', 'modal', 'prompt_id_2']
----------- 42 en bigscience/bloom-560m SA modal 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 364.08it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 3 and batchsize 16: 0:03:33.620557
path ['42', 'en', 'bloom', 'SA', 'modal', 'prompt_id_3']
----------- 42 en bigscience/bloom-560m SA modal 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 356.65it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 4 and batchsize 16: 0:03:34.250581
path ['42', 'en', 'bloom', 'SA', 'modal', 'prompt_id_4']
----------- 42 en bigscience/bloom-560m SA modal 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 362.94it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 5 and batchsize 16: 0:03:05.176310
path ['42', 'en', 'bloom', 'SA', 'modal', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloom-560m SA common 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 322.51it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 0 and batchsize 16: 0:03:03.357596
path ['42', 'en', 'bloom', 'SA', 'common', 'prompt_id_0']
----------- 42 en bigscience/bloom-560m SA common 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 322.68it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 1 and batchsize 16: 0:03:05.285701
path ['42', 'en', 'bloom', 'SA', 'common', 'prompt_id_1']
----------- 42 en bigscience/bloom-560m SA common 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 350.69it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 2 and batchsize 16: 0:03:06.169557
path ['42', 'en', 'bloom', 'SA', 'common', 'prompt_id_2']
----------- 42 en bigscience/bloom-560m SA common 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 321.35it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 3 and batchsize 16: 0:03:04.984235
path ['42', 'en', 'bloom', 'SA', 'common', 'prompt_id_3']
----------- 42 en bigscience/bloom-560m SA common 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 327.80it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 4 and batchsize 16: 0:03:02.940865
path ['42', 'en', 'bloom', 'SA', 'common', 'prompt_id_4']
----------- 42 en bigscience/bloom-560m SA common 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 347.41it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 5 and batchsize 16: 0:03:03.367323
path ['42', 'en', 'bloom', 'SA', 'common', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloom-560m SA rare_synonyms 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 353.80it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 0 and batchsize 16: 0:03:03.877134
path ['42', 'en', 'bloom', 'SA', 'rare_synonyms', 'prompt_id_0']
----------- 42 en bigscience/bloom-560m SA rare_synonyms 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 353.59it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 1 and batchsize 16: 0:03:04.326317
path ['42', 'en', 'bloom', 'SA', 'rare_synonyms', 'prompt_id_1']
----------- 42 en bigscience/bloom-560m SA rare_synonyms 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 377.58it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 2 and batchsize 16: 0:03:02.462392
path ['42', 'en', 'bloom', 'SA', 'rare_synonyms', 'prompt_id_2']
----------- 42 en bigscience/bloom-560m SA rare_synonyms 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 362.40it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 3 and batchsize 16: 0:03:04.630489
path ['42', 'en', 'bloom', 'SA', 'rare_synonyms', 'prompt_id_3']
----------- 42 en bigscience/bloom-560m SA rare_synonyms 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 377.94it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 4 and batchsize 16: 0:03:03.696555
path ['42', 'en', 'bloom', 'SA', 'rare_synonyms', 'prompt_id_4']
----------- 42 en bigscience/bloom-560m SA rare_synonyms 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 355.00it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 5 and batchsize 16: 0:03:01.299637
path ['42', 'en', 'bloom', 'SA', 'rare_synonyms', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloom-560m SA identical_modal 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 350.97it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 0 and batchsize 16: 0:03:03.429706
path ['42', 'en', 'bloom', 'SA', 'identical_modal', 'prompt_id_0']
----------- 42 en bigscience/bloom-560m SA identical_modal 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 351.96it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 1 and batchsize 16: 0:03:03.129994
path ['42', 'en', 'bloom', 'SA', 'identical_modal', 'prompt_id_1']
----------- 42 en bigscience/bloom-560m SA identical_modal 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 353.03it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 2 and batchsize 16: 0:03:03.993231
path ['42', 'en', 'bloom', 'SA', 'identical_modal', 'prompt_id_2']
----------- 42 en bigscience/bloom-560m SA identical_modal 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 343.82it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 3 and batchsize 16: 0:03:03.655731
path ['42', 'en', 'bloom', 'SA', 'identical_modal', 'prompt_id_3']
----------- 42 en bigscience/bloom-560m SA identical_modal 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 321.20it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 4 and batchsize 16: 0:03:02.446500
path ['42', 'en', 'bloom', 'SA', 'identical_modal', 'prompt_id_4']
----------- 42 en bigscience/bloom-560m SA identical_modal 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 365.84it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 5 and batchsize 16: 0:03:05.508361
path ['42', 'en', 'bloom', 'SA', 'identical_modal', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
Loading model bigscience/bloomz-560m
Model bigscience/bloomz-560m loaded
Available device is cuda
Model device: cuda:0
----------- 42 en bigscience/bloomz-560m SA active 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 204.93it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 0 and batchsize 16: 0:03:02.584826
path ['42', 'en', 'bloomz', 'SA', 'active', 'prompt_id_0']
----------- 42 en bigscience/bloomz-560m SA active 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 372.88it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 1 and batchsize 16: 0:03:34.926352
path ['42', 'en', 'bloomz', 'SA', 'active', 'prompt_id_1']
----------- 42 en bigscience/bloomz-560m SA active 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 327.94it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 2 and batchsize 16: 0:03:34.456771
path ['42', 'en', 'bloomz', 'SA', 'active', 'prompt_id_2']
----------- 42 en bigscience/bloomz-560m SA active 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 330.62it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 3 and batchsize 16: 0:03:35.784739
path ['42', 'en', 'bloomz', 'SA', 'active', 'prompt_id_3']
----------- 42 en bigscience/bloomz-560m SA active 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 314.99it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 4 and batchsize 16: 0:03:34.842667
path ['42', 'en', 'bloomz', 'SA', 'active', 'prompt_id_4']
----------- 42 en bigscience/bloomz-560m SA active 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 365.93it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type active, variation 5 and batchsize 16: 0:03:35.893836
path ['42', 'en', 'bloomz', 'SA', 'active', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloomz-560m SA passive 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 305.61it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 0 and batchsize 16: 0:03:34.230859
path ['42', 'en', 'bloomz', 'SA', 'passive', 'prompt_id_0']
----------- 42 en bigscience/bloomz-560m SA passive 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 372.06it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 1 and batchsize 16: 0:03:35.737371
path ['42', 'en', 'bloomz', 'SA', 'passive', 'prompt_id_1']
----------- 42 en bigscience/bloomz-560m SA passive 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 368.45it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 2 and batchsize 16: 0:03:34.440808
path ['42', 'en', 'bloomz', 'SA', 'passive', 'prompt_id_2']
----------- 42 en bigscience/bloomz-560m SA passive 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 193.46it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 3 and batchsize 16: 0:03:03.196597
path ['42', 'en', 'bloomz', 'SA', 'passive', 'prompt_id_3']
----------- 42 en bigscience/bloomz-560m SA passive 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 191.83it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 4 and batchsize 16: 0:03:01.414334
path ['42', 'en', 'bloomz', 'SA', 'passive', 'prompt_id_4']
----------- 42 en bigscience/bloomz-560m SA passive 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 334.87it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type passive, variation 5 and batchsize 16: 0:03:32.420842
path ['42', 'en', 'bloomz', 'SA', 'passive', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloomz-560m SA auxiliary 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 338.86it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 0 and batchsize 16: 0:03:33.858000
path ['42', 'en', 'bloomz', 'SA', 'auxiliary', 'prompt_id_0']
----------- 42 en bigscience/bloomz-560m SA auxiliary 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 350.45it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 1 and batchsize 16: 0:03:33.654802
path ['42', 'en', 'bloomz', 'SA', 'auxiliary', 'prompt_id_1']
----------- 42 en bigscience/bloomz-560m SA auxiliary 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 368.73it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 2 and batchsize 16: 0:03:33.845032
path ['42', 'en', 'bloomz', 'SA', 'auxiliary', 'prompt_id_2']
----------- 42 en bigscience/bloomz-560m SA auxiliary 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 319.51it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 3 and batchsize 16: 0:03:32.301506
path ['42', 'en', 'bloomz', 'SA', 'auxiliary', 'prompt_id_3']
----------- 42 en bigscience/bloomz-560m SA auxiliary 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 352.99it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 4 and batchsize 16: 0:03:33.032568
path ['42', 'en', 'bloomz', 'SA', 'auxiliary', 'prompt_id_4']
----------- 42 en bigscience/bloomz-560m SA auxiliary 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 335.81it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type auxiliary, variation 5 and batchsize 16: 0:03:32.017732
path ['42', 'en', 'bloomz', 'SA', 'auxiliary', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloomz-560m SA modal 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 336.88it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 0 and batchsize 16: 0:03:01.016711
path ['42', 'en', 'bloomz', 'SA', 'modal', 'prompt_id_0']
----------- 42 en bigscience/bloomz-560m SA modal 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 318.61it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 1 and batchsize 16: 0:03:04.967803
path ['42', 'en', 'bloomz', 'SA', 'modal', 'prompt_id_1']
----------- 42 en bigscience/bloomz-560m SA modal 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 366.17it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 2 and batchsize 16: 0:03:03.705223
path ['42', 'en', 'bloomz', 'SA', 'modal', 'prompt_id_2']
----------- 42 en bigscience/bloomz-560m SA modal 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 317.79it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 3 and batchsize 16: 0:03:03.785265
path ['42', 'en', 'bloomz', 'SA', 'modal', 'prompt_id_3']
----------- 42 en bigscience/bloomz-560m SA modal 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 331.66it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 4 and batchsize 16: 0:03:03.039509
path ['42', 'en', 'bloomz', 'SA', 'modal', 'prompt_id_4']
----------- 42 en bigscience/bloomz-560m SA modal 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 318.51it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type modal, variation 5 and batchsize 16: 0:03:02.548844
path ['42', 'en', 'bloomz', 'SA', 'modal', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloomz-560m SA common 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 329.21it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 0 and batchsize 16: 0:03:04.679591
path ['42', 'en', 'bloomz', 'SA', 'common', 'prompt_id_0']
----------- 42 en bigscience/bloomz-560m SA common 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 324.88it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 1 and batchsize 16: 0:03:01.201538
path ['42', 'en', 'bloomz', 'SA', 'common', 'prompt_id_1']
----------- 42 en bigscience/bloomz-560m SA common 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 353.09it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 2 and batchsize 16: 0:03:02.251475
path ['42', 'en', 'bloomz', 'SA', 'common', 'prompt_id_2']
----------- 42 en bigscience/bloomz-560m SA common 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 342.01it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 3 and batchsize 16: 0:03:03.522662
path ['42', 'en', 'bloomz', 'SA', 'common', 'prompt_id_3']
----------- 42 en bigscience/bloomz-560m SA common 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 333.52it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 4 and batchsize 16: 0:03:01.386371
path ['42', 'en', 'bloomz', 'SA', 'common', 'prompt_id_4']
----------- 42 en bigscience/bloomz-560m SA common 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 337.18it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type common, variation 5 and batchsize 16: 0:02:59.839720
path ['42', 'en', 'bloomz', 'SA', 'common', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloomz-560m SA rare_synonyms 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 335.95it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 0 and batchsize 16: 0:03:02.199866
path ['42', 'en', 'bloomz', 'SA', 'rare_synonyms', 'prompt_id_0']
----------- 42 en bigscience/bloomz-560m SA rare_synonyms 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 368.73it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 1 and batchsize 16: 0:03:02.743984
path ['42', 'en', 'bloomz', 'SA', 'rare_synonyms', 'prompt_id_1']
----------- 42 en bigscience/bloomz-560m SA rare_synonyms 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 356.32it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 2 and batchsize 16: 0:03:01.412742
path ['42', 'en', 'bloomz', 'SA', 'rare_synonyms', 'prompt_id_2']
----------- 42 en bigscience/bloomz-560m SA rare_synonyms 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 339.43it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 3 and batchsize 16: 0:03:00.922259
path ['42', 'en', 'bloomz', 'SA', 'rare_synonyms', 'prompt_id_3']
----------- 42 en bigscience/bloomz-560m SA rare_synonyms 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 366.85it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 4 and batchsize 16: 0:03:01.729623
path ['42', 'en', 'bloomz', 'SA', 'rare_synonyms', 'prompt_id_4']
----------- 42 en bigscience/bloomz-560m SA rare_synonyms 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 346.39it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type rare_synonyms, variation 5 and batchsize 16: 0:03:01.664237
path ['42', 'en', 'bloomz', 'SA', 'rare_synonyms', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
----------- 42 en bigscience/bloomz-560m SA identical_modal 0 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 374.40it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 0 and batchsize 16: 0:03:02.775284
path ['42', 'en', 'bloomz', 'SA', 'identical_modal', 'prompt_id_0']
----------- 42 en bigscience/bloomz-560m SA identical_modal 1 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 351.32it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 1 and batchsize 16: 0:03:02.183392
path ['42', 'en', 'bloomz', 'SA', 'identical_modal', 'prompt_id_1']
----------- 42 en bigscience/bloomz-560m SA identical_modal 2 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 352.42it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 2 and batchsize 16: 0:03:32.566479
path ['42', 'en', 'bloomz', 'SA', 'identical_modal', 'prompt_id_2']
----------- 42 en bigscience/bloomz-560m SA identical_modal 3 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 363.08it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 3 and batchsize 16: 0:03:34.456275
path ['42', 'en', 'bloomz', 'SA', 'identical_modal', 'prompt_id_3']
----------- 42 en bigscience/bloomz-560m SA identical_modal 4 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 309.98it/s]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 4 and batchsize 16: 0:03:34.687571
path ['42', 'en', 'bloomz', 'SA', 'identical_modal', 'prompt_id_4']
----------- 42 en bigscience/bloomz-560m SA identical_modal 5 200 16 --------------
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 336.29it/s]
MARC dataset loaded
Average length of review_body for rows with 1 star: 170.985525
Average length of review_body for rows with 5 star: 151.0092
len of lowest cat:  38553
len of pos_reviews, neg_reviews:  38553 38553
len dataset  77106
len dataset  200
Batch: 0 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 1 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 2 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 3 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 4 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 5 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 6 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 7 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 8 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 9 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 10 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 11 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
Batch: 12 , batch size: 16, sample_size: 200
answer  {'input_ids': [18260], 'attention_mask': [1]}
answer  {'input_ids': [1936], 'attention_mask': [1]}
answers_probs: tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
acc:  0.5
Time taken to execute the en SA task with prompt type identical_modal, variation 5 and batchsize 16: 0:03:32.631623
path ['42', 'en', 'bloomz', 'SA', 'identical_modal', 'prompt_id_5']
Dictionary saved to './ATCS_group3/saved_outputs/logits_dict_seed_42_lang_en_v5.pickle' as a pickle file.
****End Time: 2023-05-22 05:38:15.294648 Duraction: 4:36:28.142936
