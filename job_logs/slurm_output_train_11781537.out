----------- llama SA --------------
pad token added
Loading model huggyllama/llama-7b
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.93s/it]
Found cached dataset amazon_reviews_multi (/home/lcur1101/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)
Model huggyllama/llama-7b loaded
Moving model to cuda
Loading MARC dataset for en
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 173.85it/s]
MARC dataset loaded
len dataset  4
Batch: 0 , batch size: 2, sample_size: 4
answer  {'input_ids': [1, 4874], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}
id: [1, 4874] -> [4874]
answer  {'input_ids': [1, 694], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}
id: [1, 694] -> [694]
Logits saved to saved_logits/huggyllama/llama-7b__SA__2.txt
pred_answer ['yes', 'yes'] , label: ('yes', 'no')
Batch: 1 , batch size: 2, sample_size: 4
answer  {'input_ids': [1, 4874], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}
id: [1, 4874] -> [4874]
answer  {'input_ids': [1, 694], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}
id: [1, 694] -> [694]
Logits saved to saved_logits/huggyllama/llama-7b__SA__2.txt
pred_answer ['yes', 'yes'] , label: ('no', 'yes')
acc:  0.5
